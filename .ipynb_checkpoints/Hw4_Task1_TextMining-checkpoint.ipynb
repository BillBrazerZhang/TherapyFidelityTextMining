{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import urllib.request\n",
    "import scipy.optimize\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "\n",
    "#nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def parseData(fname):\n",
    "    for l in urllib.request.urlopen(fname):\n",
    "        yield eval(l)\n",
    "\n",
    "stopWords = set(stopwords.words(\"english\"))\n",
    "\n",
    "### Just the first 5000 reviews\n",
    "\n",
    "print(\"Reading data...\")\n",
    "data = list(parseData(\"http://jmcauley.ucsd.edu/cse190/data/beer/beer_50000.json\"))[:5000]\n",
    "print(\"done\")\n",
    "\n",
    "### Ignore capitalization and remove punctuation, and split into bigrams\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    size = len(r.split())\n",
    "    for i in range(size-1):\n",
    "        w1 = r.split()[i]\n",
    "        w2 = r.split()[i+1]\n",
    "        w1 = stemmer.stem(w1) # with stemming\n",
    "        w2 = stemmer.stem(w2) # with stemming\n",
    "#        if not (w1 in stopWords and w2 in stopWords):\n",
    "        w = w1 + ' ' + w2\n",
    "        wordCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 162557 unique bigrams amongst all 5000 reviews\n",
      "No.1: Word [with a] occurs 4587 times in the corpus\n",
      "No.2: Word [in the] occurs 2595 times in the corpus\n",
      "No.3: Word [of the] occurs 2245 times in the corpus\n",
      "No.4: Word [is a] occurs 2056 times in the corpus\n",
      "No.5: Word [on the] occurs 2033 times in the corpus\n"
     ]
    }
   ],
   "source": [
    "### Just take the most popular words...\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "num = len(counts)\n",
    "times = [x[0] for x in counts[:5]]\n",
    "words = [x[1] for x in counts[:5]]\n",
    "\n",
    "print(\"There are \" + str(num) + \" unique bigrams amongst all 5000 reviews\")\n",
    "for n in range(5):\n",
    "    i = times[n]\n",
    "    j = words[n]\n",
    "    print(\"No.\" + str(n+1) + \": Word [\"+ j + \"] occurs \" + str(i) + \" times in the corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of the prediction base on the 1000 most common bigrams is 0.340302362439\n"
     ]
    }
   ],
   "source": [
    "### Just take the most popular words...\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "times = [x[0] for x in counts[:1000]]\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "### Sentiment analysis\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    size = len(r.split())\n",
    "    for i in range(size-1):\n",
    "        w1 = r.split()[i]\n",
    "        w2 = r.split()[i+1]\n",
    "        w1 = stemmer.stem(w1) # with stemming\n",
    "        w2 = stemmer.stem(w2) # with stemming\n",
    "        w = w1 + ' ' + w2\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "#With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "num = len(predictions)\n",
    "MSE = 0\n",
    "for i in range(num):\n",
    "    MSE += (y[i]-predictions[i])**2\n",
    "mse = MSE/num\n",
    "print(\"MSE of the prediction base on the 1000 most common bigrams is\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of the prediction base on the 1000 most common bigrams is 0.312846481728\n"
     ]
    }
   ],
   "source": [
    "### Ignore capitalization and remove punctuation, and split into bigrams\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in data:\n",
    "    r = ''.join([c for c in d['review/text'].lower() if not c in punctuation])\n",
    "    size = len(r.split())\n",
    "    for w in r.split():\n",
    "        w = stemmer.stem(w) # with stemming\n",
    "        wordCount[w] += 1\n",
    "    for i in range(size-1):\n",
    "        w1 = r.split()[i]\n",
    "        w2 = r.split()[i+1]\n",
    "        w1 = stemmer.stem(w1) # with stemming\n",
    "        w2 = stemmer.stem(w2) # with stemming\n",
    "#        if not (w1 in stopWords and w2 in stopWords):\n",
    "        w = w1 + ' ' + w2\n",
    "        wordCount[w] += 1\n",
    "        \n",
    "### Just take the most popular words...\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "times = [x[0] for x in counts[:1000]]\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "\n",
    "### Sentiment analysis\n",
    "\n",
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)\n",
    "\n",
    "def feature(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['review/text'].lower() if not c in punctuation])\n",
    "    size = len(r.split())\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    for i in range(size-1):\n",
    "        w1 = r.split()[i]\n",
    "        w2 = r.split()[i+1]\n",
    "        w1 = stemmer.stem(w1) # with stemming\n",
    "        w2 = stemmer.stem(w2) # with stemming\n",
    "        w = w1 + ' ' + w2\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) #offset\n",
    "    return feat\n",
    "\n",
    "X = [feature(d) for d in data]\n",
    "y = [d['review/overall'] for d in data]\n",
    "\n",
    "#With regularization\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False)\n",
    "clf.fit(X, y)\n",
    "theta = clf.coef_\n",
    "predictions = clf.predict(X)\n",
    "num = len(predictions)\n",
    "MSE = 0\n",
    "for i in range(num):\n",
    "    MSE += (y[i]-predictions[i])**2\n",
    "mse = MSE/num\n",
    "print(\"MSE of the prediction base on the 1000 most common bigrams is\",mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 5 unigrams/bigrams with the most positive associated weights and their weights are listed as:\n",
      "wa------weight: 0.385810095911\n",
      "impress------weight: 0.312095684774\n",
      "the best------weight: 0.25788118686\n",
      "quit------weight: 0.243636388717\n",
      "not too------weight: 0.238452957975\n",
      "The 5 unigrams/bigrams with the most negative associated weights and their weights are listed as:\n",
      "coffe------weight: -0.323729455127\n",
      "corn------weight: -0.281909925982\n",
      "water------weight: -0.277372578656\n",
      "carbon------weight: -0.250541574282\n",
      "straw------weight: -0.237242201396\n"
     ]
    }
   ],
   "source": [
    "Impact = [(theta[wordId[w]], w) for w in words]\n",
    "Impact.sort()\n",
    "Impact.reverse()\n",
    "print(\"The 5 unigrams/bigrams with the most positive associated weights and their weights are listed as:\")\n",
    "for i in range(5):\n",
    "    print(Impact[i][1] + \"------weight: \" + str(Impact[i][0]))\n",
    "print(\"The 5 unigrams/bigrams with the most negative associated weights and their weights are listed as:\")\n",
    "for i in range(5):\n",
    "    print(Impact[len(Impact)-i-1][1] + \"------weight: \" + str(Impact[len(Impact)-i-1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
